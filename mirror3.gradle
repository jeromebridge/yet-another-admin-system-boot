apply plugin: 'maven'
version = '1.0.0'

buildscript {
    repositories {
        mavenCentral()
        maven { url "http://repo.pennassurancesoftware.com/artifactory/public/" }
    }
        
    dependencies {
        classpath "edu.uci.ics:crawler4j:4.1"
    }
}

configurations {
    allJars
}

ext {
	amdatuUrl = 'http://repository.amdatu.org/dependencies'
	repoUrl = 'http://repo.pennassurancesoftware.com/artifactory/amdatu-dependencies-mirror'
	groupId = "org.test"
}

task uploadDependencies( dependsOn: [ 'uploadAllJars' ] ) << {}
task uploadReleases( dependsOn: [ 'uploadAllJars' ] ) << {}
task uploadPasReleases( dependsOn: [ 'uploadAllJars' ] ) << {}
task mirrorDependencies ( dependsOn: [ 'mirror' ] ) << {}
task mirrorReleases ( dependsOn: [ 'mirror' ] ) << {}
task mirrorPasReleases ( dependsOn: [ 'mirror' ] ) << {}
task mirrorAndUploadDependencies ( dependsOn: [ 'mirrorDependencies', 'uploadDependencies' ] ) << {}
task mirrorAndUploadReleases ( dependsOn: [ 'mirrorReleases', 'uploadReleases' ] ) << {}
task mirrorAndUploadPasReleases ( dependsOn: [ 'mirrorPasReleases', 'uploadPasReleases' ] ) << {}

task mirror << {
	crawl( amdatuUrl, "build/mirror" );
}

gradle.taskGraph.whenReady {taskGraph ->
    if( taskGraph.hasTask( uploadDependencies ) ) {
        project.ext.repoUrl = 'http://repo.pennassurancesoftware.com/artifactory/amdatu-dependencies-mirror'
		project.ext.groupId = "org.amdatu.dependency"
    } else if( taskGraph.hasTask( uploadReleases ) ) {
        project.ext.repoUrl = 'http://repo.pennassurancesoftware.com/artifactory/amdatu-releases-mirror'
		project.ext.groupId = "org.amdatu"
    } else if( taskGraph.hasTask( uploadPasReleases ) ) {
        project.ext.repoUrl = 'http://repo.pennassurancesoftware.com/artifactory/pennassurancesoftware-releases-mirror'
		project.ext.groupId = "com.pennassurancesoftware"
    }
    
    
    if( taskGraph.hasTask( mirrorDependencies ) ) {
    	project.ext.amdatuUrl = 'http://repository.amdatu.org/dependencies'
    } else if( taskGraph.hasTask( mirrorReleases ) ) {
    	project.ext.amdatuUrl = 'http://repository.amdatu.org/release'
    } else if( taskGraph.hasTask( mirrorPasReleases ) ) {
    	project.ext.amdatuUrl = 'http://repo3.pennassurancesoftware.com/content/repositories/osgi-releases'
    }
}

uploadAllJars {
   repositories {  
        mavenDeployer {  
            repository( url: repoUrl ) {  
                authentication( userName: 'jbc', password: 'london10' );  
            }
       }
   }
}

task discoverAllJars{
    ext.discoveredFiles = []
    doLast{
        fileTree( "build/mirror" ).matching { include "**/*.jar" }.each { File file ->
            println "found file ${file.name}" 
            discoveredFiles << file
            artifacts{
                allJars file
            }
        }   
    }
}

task configureUploadAllJars {
    dependsOn discoverAllJars
    doLast{
        uploadAllJars {
           repositories {  
                mavenDeployer {  
                    repository( url: repoUrl ) {  
		                authentication( userName: 'jbc', password: 'london10' );
		            }
                    discoverAllJars.discoveredFiles.each{ discoveredFile ->
                        def filterName = discoveredFile.name - ".jar"
                        def fbase = discoveredFile.name.lastIndexOf('.').with {it != -1 ? discoveredFile.name[0..<it] : discoveredFile.name}
			    		def artifactId = discoveredFile.name.split("-")[0]
			    		def versionNum = fbase.split("-").last()
                        addFilter(filterName) { artifact, file ->
                            file.name == discoveredFile.name
                        }
                        pom(filterName).artifactId = artifactId
                        pom(filterName).groupId = groupId
                        pom(filterName).version = versionNum
                    }
                }  
            }  
        }   
    }
}

uploadAllJars.dependsOn configureUploadAllJars
uploadAllJars.mustRunAfter mirror


import edu.uci.ics.crawler4j.crawler.WebCrawler;
import edu.uci.ics.crawler4j.crawler.Page;
import java.util.regex.Pattern;
import edu.uci.ics.crawler4j.url.WebURL;
import edu.uci.ics.crawler4j.parser.HtmlParseData;
import edu.uci.ics.crawler4j.parser.BinaryParseData;
import edu.uci.ics.crawler4j.crawler.CrawlController;
import java.util.UUID;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;

public class MyCrawler extends WebCrawler {
    private final static Pattern FILTERS = Pattern.compile( '.*(\\.(css|js|mid|mp2|mp3|mp4|wav|avi|mov|mpeg|ram|m4v|pdf|rm|smil|wmv|swf|wma|zip|rar|gz))$' );
    private final static Pattern JAR_PATTERNS = Pattern.compile( '.*(\\.(jar))$' );
	
	private final String baseUrl;
	private final File storageFolder;
	
	public MyCrawler( String baseUrl, File storageFolder ) {
		this.baseUrl = baseUrl;
		this.storageFolder= storageFolder;
		if ( !storageFolder.exists() ) {
	    	storageFolder.mkdirs();
	    }
	}

    /**
     * This method receives two parameters. The first parameter is the page
     * in which we have discovered this new url and the second parameter is
     * the new url. You should implement this function to specify whether
     * the given url should be crawled or not (based on your crawling logic).
     * In this example, we are instructing the crawler to ignore urls that
     * have css, js, git, ... extensions and to only accept urls that start
     * with base URL. In this case, we didn't need the
     * referringPage parameter to make the decision.
     */
     @Override
     public boolean shouldVisit(Page referringPage, WebURL url) {
	    final String href = url.getURL().toLowerCase();
	    boolean result = !FILTERS.matcher( href ).matches();
	    result = result && ( href.startsWith( baseUrl ) || JAR_PATTERNS.matcher( href ).matches() );
		return result;
     }

     /**
      * This function is called when a page is fetched and ready
      * to be processed by your program.
      */
     @Override
     public void visit(Page page) {
     	String url = page.getWebURL().getURL();
     	if( JAR_PATTERNS.matcher(url).matches() && page.getParseData() instanceof BinaryParseData ) {
     		final String extension = url.substring( url.lastIndexOf('.') );
     		final String fileName = url.substring( url.lastIndexOf( '/' ) );
     		String pathName = url.substring( baseUrl.length(), url.lastIndexOf( '/' ) );
     		pathName = pathName.startsWith( "/") ? pathName.substring( 1 ) : pathName;
            new File( "${storageFolder.absolutePath}/$pathName" ).mkdirs();
            final Path path = Paths.get( storageFolder.absolutePath, pathName, fileName );
            try {
      			Files.write( path, page.getContentData() );
      			println "Stored: $path";
    		} catch (IOException iox) {
      			println "Failed to write file: $path reason: $iox";
    		}
     	}  
     }
}


import edu.uci.ics.crawler4j.crawler.CrawlConfig;
import edu.uci.ics.crawler4j.fetcher.PageFetcher;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtConfig;
import edu.uci.ics.crawler4j.robotstxt.RobotstxtServer;
import edu.uci.ics.crawler4j.crawler.CrawlController;
def crawl( url, dest ) {
	delete dest

    def MEGABYTE = 1024L * 1024L;
	def numberOfCrawlers = 7;
    def config = new CrawlConfig();
    config.crawlStorageFolder = "$dest/tmp";
    config.maxDownloadSize = 100L * MEGABYTE; // 100MB
    config.includeBinaryContentInCrawling = true;

    /*
     * Instantiate the controller for this crawl.
     */
    PageFetcher pageFetcher = new PageFetcher(config);
    RobotstxtConfig robotstxtConfig = new RobotstxtConfig();
    RobotstxtServer robotstxtServer = new RobotstxtServer(robotstxtConfig, pageFetcher);
    CrawlController controller = new CrawlController(config, pageFetcher, robotstxtServer);

    /*
     * For each crawl, you need to add some seed urls. These are the first
     * URLs that are fetched and then the crawler starts following links
     * which are found in these pages
     */
    controller.addSeed( url );
    // controller.addSeed("http://www.ics.uci.edu/~welling/");
    // controller.addSeed("http://www.ics.uci.edu/");

    /*
     * Start the crawl. This is a blocking operation, meaning that your code
     * will reach the line after this only when crawling is finished.
     */
    def num = Math.abs(new Random().nextInt())
    def className = "Crawler$num"
    def classLoader = new GroovyClassLoader();
    def crawlerClass = classLoader.parseClass( "public class $className extends MyCrawler{ public $className(){super( \"$url\", new java.io.File( \"$dest/download\") ); } }" )  
    
    controller.start( crawlerClass, numberOfCrawlers );

}